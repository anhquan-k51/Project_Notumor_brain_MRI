# -*- coding: utf-8 -*-
"""Classification_mobilenetv2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/158AUUY8g0MaRazoeBU1IP_T1wJPTS3pw
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Import thư viện"""

import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf

from tensorflow.keras.preprocessing import image_dataset_from_directory

"""## Config một số tham số"""

batch_size = 32
input_size = 160
model_name = "mobilenet_V2"
version =  1
initial_epochs = 50
base_learning_rate = 0.0001

path_colab_drive =  '/content/drive/MyDrive/20.05.2023/classification/'

!cp -R "{path_colab_drive}/data" /content/data
!unzip /content/data/data_end.zip
!rm /content/data/data_end.zip

"""# Xóa ảnh lỗi"""

import os
from PIL import Image
import cv2

count = 0
list_foler = ["/content/data/Training/", "/content/data/Testing/"]
for folder in list_foler:
    for class_folder in os.listdir(folder):
        folder_path = os.path.join(folder, class_folder)
        flag_break = False
        for img_file in os.listdir(folder_path):
            old_img_file_path = os.path.join(folder_path, img_file)
            filename, ext = os.path.splitext(img_file)
            new_img_file_path = os.path.join(folder_path, filename + '.jpg')
            try:
                new_img = Image.open(old_img_file_path)
                new_img.save(new_img_file_path, format="JPEG", quality=100)
                if old_img_file_path != new_img_file_path:
                    new_img.close()
                    os.remove(old_img_file_path)
            except Exception as e:
                new_img.close()
                os.remove(old_img_file_path)
                print("Remove: ",old_img_file_path)
                count += 1
                continue
print("Number of data remove is: ", count)

"""## Xử lý ảnh: đưa ảnh về đúng kích thước (160, 160)"""

train_dir = '/content/data/Training'
validation_dir = test_dir = '/content/data/Testing'
BATCH_SIZE = batch_size
IMG_SIZE = (input_size, input_size)

train_dataset = image_dataset_from_directory(train_dir,
                                             shuffle=True,
                                             batch_size=BATCH_SIZE,
                                             image_size=IMG_SIZE)

validation_dataset = image_dataset_from_directory(validation_dir,
                                                  shuffle=True,
                                                  batch_size=BATCH_SIZE,
                                                  image_size=IMG_SIZE)
test_dataset = image_dataset_from_directory(test_dir,
                                                  shuffle=True,
                                                  batch_size=BATCH_SIZE,
                                                  image_size=IMG_SIZE)
class_names = test_dataset.class_names

"""### Bộ dữ liệu phân loại các loại u nao

*   List item
*   List item


"""

class_names = train_dataset.class_names
print("Các class: ", class_names)
num_classes = len(class_names)
print("Số lượng class: ", num_classes)

plt.figure(figsize=(10, 10))
for images, labels in train_dataset.take(1):
  for i in range(9):
    try:
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(class_names[labels[i]])
        plt.axis("off")
    except:
        continue

"""### Tiếp tục xử lý ảnh như: xoay, làm nhiễu,..."""

AUTOTUNE = tf.data.experimental.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)

data_augmentation = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal_and_vertical'),
  tf.keras.layers.experimental.preprocessing.RandomContrast(1),
  tf.keras.layers.experimental.preprocessing.RandomRotation((-1, 0.5)),
  tf.keras.layers.experimental.preprocessing.RandomCrop(160,160),
  tf.keras.layers.GaussianNoise(0.2)
])

"""## Xây dựng mô hình

### Pretrained
"""

preprocess_input = tf.keras.applications.mobilenet.preprocess_input
model_type = tf.keras.applications.MobileNetV2

IMG_SHAPE = IMG_SIZE + (3,)
base_model = model_type(input_shape=IMG_SHAPE,
                        include_top=False,
                        weights='imagenet')

try:
    image_batch, label_batch  = iter(train_dataset)
except:
    image_batch, label_batch  = next(iter(train_dataset))

image_batch, label_batch = next(iter(train_dataset))
feature_batch = base_model(image_batch)
print(feature_batch.shape)

base_model.trainable = False

# Let's take a look at the base model architecture
base_model.summary()

global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
feature_batch_average = global_average_layer(feature_batch)
print(feature_batch_average.shape)

prediction_layer = tf.keras.layers.Dense(num_classes, name='predict')
prediction_batch = prediction_layer(feature_batch_average)
print(prediction_batch.shape)

"""### Mô hình"""

inputs = tf.keras.Input(shape=IMG_SHAPE, name='image_tensor')
x = data_augmentation(inputs)
x = preprocess_input(x)
x = base_model(x, training=False)
x = global_average_layer(x)
x = tf.keras.layers.Dropout(0.2)(x)
outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs, name=model_name)

checkpoints_path = "/content/drive/MyDrive/checkpoint/"
if not os.path.exists(checkpoints_path):
    os.makedirs(checkpoints_path)

"""## Huấn luyện"""

model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.summary()
print(f"Number of trainable variables: {len(model.trainable_variables)}")

loss0, accuracy0 = model.evaluate(validation_dataset)
print("initial loss: {:.2f}".format(loss0))
print("initial accuracy: {:.2f}".format(accuracy0))

checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=os.path.join(checkpoints_path, 'model_checkpoint.h5'),
    monitor='val_accuracy',
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode='max',
    save_freq='epoch'
)
callbacks = [checkpoint_callback]

history = model.fit(train_dataset,
                    epochs=initial_epochs,
                    validation_data=validation_dataset,
                    callbacks=callbacks)

accurary = max(history.history['val_accuracy'])
print("Đô chính xác: ", accurary)

train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
train_loss = history.history['loss']
val_loss = history.history['val_loss']

# Vẽ biểu đồ độ chính xác
plt.plot(train_acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()

# Vẽ biểu đồ mất mát
plt.plot(train_loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')
plt.show()

train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
train_loss = history.history['loss']
val_loss = history.history['val_loss']

# Vẽ biểu đồ độ chính xác
plt.plot(train_acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()

# Vẽ biểu đồ mất mát
plt.plot(train_loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')
plt.show()

"""### Lưu mô hình"""

model.save(f'{path_colab_drive}/model/model.h5', include_optimizer=True)

"""# Đánh giá"""

from tensorflow.keras.models import load_model
from google.colab.patches import cv2_imshow
import os
import cv2
import tensorflow as tf
import numpy as np

def evulate(data_end):
    class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']

    # load model
    model = load_model('/content/drive/MyDrive/20.05.2023/classification/model/model.h5')

    result = []
    for img_path in data_end:
      img_arr = cv2.imread(img_path)
      np_img = np.array(img_arr)

      # print(np_img.shape)
      image = np.expand_dims(cv2.resize(np.squeeze(np_img), (160, 160)), axis=0)

      predictions = model.predict(image)
      scores = tf.nn.sigmoid(predictions) # 4 > 100%
      pred_labels = np.argmax(scores, axis=-1)
      result.append(int(pred_labels))
    return result

data_dir = '/content/drive/MyDrive/20.05.2023/classification/data/data_end/Testing'  # Đường dẫn tới thư mục chứa dữ liệu
class_names = os.listdir('/content/drive/MyDrive/20.05.2023/classification/data/data_end/Testing')  # Lấy danh sách tên các lớp từ tên thư mục
image_size = (64, 64)  # Kích thước ảnh đầu vào
test_size = 0.2  # Tỉ lệ dữ liệu dùng để kiểm tra

data_end = []
labels = []

for class_name in class_names:
    class_dir = os.path.join(data_dir, class_name)
    for image_name in os.listdir(class_dir):
        image_path = os.path.join(class_dir, image_name)
        data_end.append(image_path)
        labels.append(class_name)

class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']

y_true = [class_names.index(x) for x in labels]

y_predict = evulate(data_end)

from sklearn.metrics import classification_report, ConfusionMatrixDisplay

target_names = ['glioma', 'meningioma', 'notumor','pituitary']

print(classification_report(y_true, y_predict, target_names=target_names))

ConfusionMatrixDisplay.from_predictions(
    y_true, y_predict
)
print("\nConfusion matrix: X-Dự Đoán \t Y-Nhãn Thật\n0-Glioma\t1-Meningioma\t\t2-Noturmor\t 3-Pitutary")

"""# Dự đoán"""

from tensorflow.keras.models import load_model
from google.colab.patches import cv2_imshow
import numpy as np
import matplotlib.pyplot as plt
import cv2

model_path = f'{path_colab_drive}/model/model.h5'

def predict_class_img_with_img(img_path):
    class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']

    # load model
    model = load_model(model_path)
    img_arr = cv2.imread(img_path)
    np_img = np.array(img_arr)

    # print(np_img.shape)
    image = np.expand_dims(cv2.resize(np.squeeze(np_img), (160, 160)), axis=0)

    predictions = model.predict(image)
    scores = tf.nn.sigmoid(predictions)
    print(scores)
    pred_labels = np.argmax(scores, axis=-1)
    print(f"Label is {class_names[int(pred_labels)]}, score is {np.max(scores) * 100}%")
    cv2_imshow(image[0])
    # return class_names[int(pred_labels)]

img_path = '/content/drive/MyDrive/20.05.2023/classification/data/data_end/Testing/glioma/Te-gl_0042.jpg'
predict_class_img_with_img(img_path)

img_path = '/content/drive/MyDrive/20.05.2023/classification/data/data_end/Testing/meningioma/Te-me_0025.jpg'
predict_class_img_with_img(img_path)

img_path = '/content/drive/MyDrive/20.05.2023/classification/data/data_end/Training/notumor/Tr-no_0597.jpg'
predict_class_img_with_img(img_path)

img_path = '/content/drive/MyDrive/20.05.2023/classification/data/data_end/Testing/pituitary/Te-pi_0157.jpg'
predict_class_img_with_img(img_path)